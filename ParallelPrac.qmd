[GitHub Link](https://github.com/NesanNaidoo/ParallelPrac)

## Q1
```{r}
#| warning: FALSE
#| loading: FALSE
#| results: asis
library(foreach)
library(doParallel)

# Set up parallel 
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

n_reps <- 100

results <- foreach(i = 1:n_reps, .combine = rbind) %dopar% {
  sample_data <- rexp(100, rate = 1)  
  sample_mean <- mean(sample_data)
  sample_variance <- var(sample_data)
  c(mean = sample_mean, variance = sample_variance)
}

# Stop parallel 
stopCluster(cl)

results_df <- as.data.frame(results)
knitr::kable(head(results_df),digits=2,col.names = c("Mean","Variance"),caption="")

```

## Q2

```{r}
#| warning: true
#| message: false
#| loading: false

library(MASS)
library(foreach)
library(doParallel)

# Set up parallel processing 
cl <- makeCluster(3)
registerDoParallel(cl)


bootstrapMedian <- function(data, n) {
  sample_data <- sample(data, size = n, replace = TRUE)
  return(median(sample_data))
}


set.seed(42)  # For reproducibility
system.time({
  bootstrapped_medians_serial <- sapply(1:1000, function(x) bootstrapMedian(galaxies, n = 82))
})

print(head(bootstrapped_medians_serial))



# Run bootstrapping in parallel
library(doParallel)
cl <- makeCluster(4)  
registerDoParallel(cl)


set.seed(42)  
system.time({
  bootstrapped_medians_parallel <- foreach(i = 1:1000, .combine = c, .packages = "MASS") %dopar% {
    bootstrapMedian(galaxies, n = 82)
  }
})

stopCluster(cl)


print(head(bootstrapped_medians_parallel))



```

For the current bootstrapping task with the galaxies dataset, serial execution is faster due to the small task size and the cost of parallelization overhead. If you were processing much larger datasets or running more complex computations, you would likely see greater benefits from parallelization.

## Q3

```{r}
set.seed(42)  # For reproducibility

original_sample <- rexp(50, rate = 1)  #


bootstrap_mean <- function(sample, n_iterations = 1000) {
  bootstrap_stats <- numeric(n_iterations)
  
  for (i in 1:n_iterations) {
    bootstrap_sample <- sample(sample, replace = TRUE)
    bootstrap_stats[i] <- mean(bootstrap_sample)  #
  }
  
  return(bootstrap_stats)
}


n_iterations <- 1000
bootstrap_stats <- bootstrap_mean(original_sample, n_iterations)


lower_percentile <- 2.5
upper_percentile <- 97.5

lower_bound <- quantile(bootstrap_stats, lower_percentile / 100)
upper_bound <- quantile(bootstrap_stats, upper_percentile / 100)


true_value <- 1

coverage <- ifelse(true_value >= lower_bound & true_value <= upper_bound, 1, 0)

n_trials <- 1000
coverage_results <- numeric(n_trials)

for (i in 1:n_trials) {
  bootstrap_stats <- bootstrap_mean(original_sample, n_iterations)
  lower_bound <- quantile(bootstrap_stats, lower_percentile / 100)
  upper_bound <- quantile(bootstrap_stats, upper_percentile / 100)
  coverage_results[i] <- ifelse(true_value >= lower_bound & true_value <= upper_bound, 1, 0)
}

coverage_estimate <- mean(coverage_results)
coverage_estimate

```

## Q4

```{r}
# Load necessary packages
library(foreach)
library(doParallel)  # For parallel processing
library(iterators)    # For irnorm function

# Set the seed for reproducibility
set.seed(1234)

# Register parallel backend (4 workers in this case)
cl <- makeCluster(4)
registerDoParallel(cl)

# Create a list of 3 vectors, each containing 5 random variables from a normal distribution
vectors <- foreach(i = 1:3, .combine = 'list', .packages = 'iterators') %dopar% {
  it <- irnorm(5)  # Create an iterator for 5 random normal variables
  unlist(sapply(1:5, function(x) nextElem(it)))  # Extract the values from the iterator
}

# Print the vectors to check
print(vectors)

# Find the largest value in each vector using foreach in parallel
largest_values <- foreach(vec = vectors, .combine = 'c') %dopar% {
  # Apply max() correctly to each vector (make sure the result is combined as a vector of max values)
  max_value <- max(unlist(vec))  
  max_value  # Return the largest value from each vector
}

# Stop the cluster after computation is done
stopCluster(cl)

# Print the largest values
print(largest_values)



```

## Q5

```{r}
#| results: asis

library(foreach)
library(doParallel)  
library(iterators)    
library(parallel)     
library(tictoc)       


set.seed(1234)

# Register parallel 
cl <- makeCluster(4)
registerDoParallel(cl)

create_vectors <- function() {
  vectors <- foreach(i = 1:3, .combine = 'list', .packages = 'iterators') %dopar% {
    it <- irnorm(5)  
    unlist(sapply(1:5, function(x) nextElem(it)))  
  }
  return(vectors)
}


tic("parLapply execution")
vectors_parLapply <- parLapply(cl, 1:3, function(i) {
  library(iterators)  
  it <- irnorm(5)
  unlist(sapply(1:5, function(x) nextElem(it)))
})
largest_values_parLapply <- sapply(vectors_parLapply, function(vec) max(unlist(vec)))
toc()


tic("foreach execution")
vectors_foreach <- create_vectors()
largest_values_foreach <- foreach(vec = vectors_foreach, .combine = 'c') %dopar% {
  max(unlist(vec))
}
toc()

tic("replicate execution")
vectors_replicate <- replicate(3, unlist(sapply(1:5, function(x) rnorm(1))), simplify = FALSE)
largest_values_replicate <- sapply(vectors_replicate, function(vec) max(vec))
toc()


stopCluster(cl)

Metrics<-c("Largest values from parLapply:","","","Largest values from foreach:","","Largest values from replicate:","","")
Values<-c(largest_values_parLapply,largest_values_foreach,largest_values_replicate)

knitr::kable(data.frame(Metrics,Values),digits = 2,caption = "Question 5")

```
